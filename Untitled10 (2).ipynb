{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":6864,"status":"ok","timestamp":1727978158944,"user":{"displayName":"Eva Vallistu","userId":"14113203656686220563"},"user_tz":-180},"id":"N_C8XlLgbMiT","outputId":"bad507da-16fb-474d-d802-ef8b4827731c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.5 alembic-1.13.3 colorlog-6.8.2 optuna-4.0.0\n"]}],"source":["pip install optuna"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ky8BQV3NfhPd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"elapsed":700,"status":"error","timestamp":1727978406235,"user":{"displayName":"Eva Vallistu","userId":"14113203656686220563"},"user_tz":-180},"id":"ileFAriAGiYa","outputId":"7aac2944-39ba-4b00-a740-c516506d4afc"},"outputs":[{"ename":"ValueError","evalue":"cannot reshape array of size 131056 into shape (71,240,419)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-2f0ebb73128c>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-2f0ebb73128c>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbeijing_train1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/task2/bnu1.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mbeijing_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/task2/bnu2.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mihb_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/task2/ihb.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    455\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    457\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                                          max_header_size=max_header_size)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 131056 into shape (71,240,419)"]}],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import optuna\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","from sklearn.decomposition import PCA\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM, Conv1D, Dropout\n","from tensorflow.keras.utils import to_categorical\n","\n","def load_data():\n","    beijing_train1 = np.load('/task2/bnu1.npy')\n","    beijing_train2 = np.load('/task2/bnu2.npy')\n","    ihb_train = np.load('/task2/ihb.npy')\n","    beijing_labels = pd.read_csv('/task2/bnu.csv', header=0).values.ravel()\n","    ihb_labels = pd.read_csv('/task2/ihb.csv', header=0).values.ravel()\n","    beijing_data = np.concatenate((beijing_train1, beijing_train2), axis=0)\n","    pca_beijing = PCA(n_components=20)\n","    beijing_data_reduced = pca_beijing.fit_transform(beijing_data.reshape(beijing_data.shape[0], -1))\n","    ihb_data_reduced = ihb_train.reshape(ihb_train.shape[0], -1)\n","    pca_ihb = PCA(n_components=20)\n","    ihb_data_reduced = pca_ihb.fit_transform(ihb_data_reduced)\n","    X = np.concatenate((beijing_data_reduced, ihb_data_reduced), axis=0)\n","    y = np.concatenate((beijing_labels, ihb_labels), axis=0)\n","    return X, y\n","\n","def preprocess_data(X):\n","    num_samples, num_features = X.shape\n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","    return X_scaled.reshape(num_samples, -1, 20)\n","\n","def compute_functional_connectivity(X):\n","    num_samples, time_steps, num_regions = X.shape\n","    connectivity_matrices = np.zeros((num_samples, num_regions, num_regions))\n","    for i in range(num_samples):\n","        connectivity_matrices[i] = np.corrcoef(X[i], rowvar=False)\n","    return connectivity_matrices\n","\n","def create_model(trial, input_shape):\n","    model = Sequential()\n","    n_conv_layers = trial.suggest_int('n_conv_layers', 1, 3)\n","    for i in range(n_conv_layers):\n","        filters = trial.suggest_categorical(f'filters_{i}', [16, 32, 64])\n","        kernel_size = trial.suggest_int(f'kernel_size_{i}', 1, min(5, input_shape[0]))\n","        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n","    n_lstm_layers = trial.suggest_int('n_lstm_layers', 1, 2)\n","    for i in range(n_lstm_layers):\n","        units = trial.suggest_categorical(f'units_{i}', [32, 64, 128])\n","        model.add(LSTM(units=units, return_sequences=(i < n_lstm_layers - 1)))\n","    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n","    model.add(Dropout(rate=dropout_rate))\n","    model.add(Dense(2, activation='softmax'))\n","    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","def objective(trial):\n","    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_categorical, test_size=0.2, stratify=y, random_state=42)\n","    input_shape = (X_train.shape[1], X_train.shape[2])\n","    model = create_model(trial, input_shape)\n","    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n","    epochs = 50\n","    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, verbose=0)\n","    score = model.evaluate(X_val, y_val, verbose=0)\n","    accuracy = score[1]\n","    return 1 - accuracy\n","if __name__ == '__main__':\n","    X, y = load_data()\n","\n","    X_scaled = preprocess_data(X)\n","\n","    y_categorical = to_categorical(y, num_classes=2)\n","\n","    study = optuna.create_study(direction='minimize')\n","    study.optimize(objective, n_trials=20)\n","\n","    # print('Наилучшие гиперпараметры:', study.best_params)\n","\n","    best_trial = study.best_trial\n","    input_shape = (X_scaled.shape[1], X_scaled.shape[2])\n","    model = create_model(best_trial, input_shape)\n","\n","    batch_size = best_trial.params['batch_size']\n","    epochs = 50\n","    model.fit(X_scaled, y_categorical, epochs=epochs, batch_size=batch_size, verbose=1)\n","\n","    model.save('fmri_classification_model.keras')\n","\n","    if os.path.exists('./data/ts_cut/HCPex/predict.npy'):\n","        X_test = np.load('./data/ts_cut/HCPex/predict.npy')\n","        X_test_scaled = preprocess_data(X_test)\n","\n","        predictions = model.predict(X_test_scaled)\n","        predicted_classes = np.argmax(predictions, axis=1)\n","\n","        np.savetxt('predictions.csv', predicted_classes, fmt='%d')\n","\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNF9Gkg9kaniwpwWCSzovdc"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}